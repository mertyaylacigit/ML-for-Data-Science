{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c68426-ddc5-46d1-a118-457ffe2056da",
   "metadata": {},
   "source": [
    "Lab ML for Data Science: Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Hey candra We are done with part 1 of part 1, I need to go. So I will close my laptop and the session will be ended.\n",
    "# I upload it to github and just pull it and do your stuff. OK? great\n",
    "\n",
    "# OK\n",
    "\n",
    "transformer =FunctionTransformer(np.log1p, validate=True)\n",
    "log_transformed_data = transformer.transform(data_scatter)\n",
    "log_transformed_df = pd.DataFrame(log_transformed_data, colum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import scipy\n",
    "#import torch\n",
    "#import torchvision\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11b8c2-f0be-43a6-ba7d-8bce6c8e3a27",
   "metadata": {},
   "source": [
    "# 1 Loading the Data, Preprocessing, Initial Data Analysis\n",
    "\n",
    "The first step will consist of **downloading the dataset and converting it into numerical tables (e.g. numpy, arrays)**.\n",
    "In practice, raw data is rarely directly usable as input to machine learning algorithms. In particular, there may be substantial heterogeneity between the different input features. Some features may be physical measurements, monetary measurements, while others may be category indicators or even non-numerical data such as text or images. Hence, a **preliminary filtering** of what is interesting for the analysis we would like to conduct is desirable.\n",
    "\n",
    "In the context of the UCI wholesale dataset, one may, for example, want to base anomaly and cluster predictions on numerical data (annual spending per category) and drop meta-data such as Channel and the Region indicators or reserve it for an ulterior use. Once such a preliminary step has been taken, we have a standard dataset of size $N × d$ where $N$ is the number of instances (wholesale customers), $d$ is the number of spending categories, and each value in the table can be expressed in monetary unit.\n",
    "\n",
    "To verify the range and distribution of these values, we can **generate some basic statistical visualizations** of the data. This includes **histograms** showing for each category the distribution of spendings, or **scatter plots** showing the correlation between different product categories. A common observation from such basic statistical analysis is that the distributions are heavy tailed, with many instances having rather small spendings, whereas a few may have spendings one or two orders of magnitude above. \n",
    "\n",
    "Any anomaly detection algorithm would systematically highlight those high spenders as anomalous and not make a distinction between spending little and not spending at all.\n",
    "To address this issue, it can be useful to apply some **nonlinear transformation** to the data, for example, applying the log function to the features so that the distribution becomes compressed for large values and expanded for small values.\n",
    "\n",
    "$x → log(x + θ)$\n",
    "\n",
    "Here, we add a positive offset $θ$ in the logarithm so that zero spending $(x = 0)$ does not get mapped to $−∞$. You may experiment with different offsets, such as $θ = 1, θ = 10$ or $θ = 100$. To verify the effect of the transformation, you can **recompute the histograms and scatter plots** in transformed space and check visually whether the transformation had the desired result, e.g. whether the distribution look Gaussian-distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7122155",
   "metadata": {},
   "source": [
    "# Loading and Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv('Wholesale customers data.csv')\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a459e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2eeb0f",
   "metadata": {},
   "source": [
    "- the dataset has no missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e37a49",
   "metadata": {},
   "source": [
    "- from such basic statistical analysis is that the distributions are heavy tailed, with many instances having rather small spendings, whereas a few may have spendings one or two orders of magnitude above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1796f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each product category\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "for i, column in enumerate(data_scatter.columns):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(data_scatter[column], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(column + ' Distribution')\n",
    "    plt.xlabel('Spending')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scatter = data_pd.iloc[:,2:]  # remove channel and region\n",
    "pd.plotting.scatter_matrix(data_scatter, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scatter.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239601e5",
   "metadata": {},
   "source": [
    "**Grocery and Detergents Paper are highly correlating. Grocery and Milk are correlating (but weaker with 73%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.groupby([\"Region\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938f3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.groupby([\"Channel\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6d83c",
   "metadata": {},
   "source": [
    "**class imbalance of region and channel** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93284904",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2a34c",
   "metadata": {},
   "source": [
    "### Log transformation to get gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 1e-10\n",
    "data_log = data_pd.apply(lambda x:\n",
    "                         np.log(x + offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38298f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_scatter = data_log.iloc[:,2:]  # remove channel and region\n",
    "pd.plotting.scatter_matrix(data_log_scatter, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20f4df",
   "metadata": {},
   "source": [
    "**Now the the features are gaussian distributed and we can continue with anomaly detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc54d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_scatter.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd.iloc[:,2].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c007c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
